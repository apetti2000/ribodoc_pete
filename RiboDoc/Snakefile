configfile: "/data/config.yaml"


ribodoc_version = "0.8.4"


# Imports
from optparse import OptionParser
import gffutils
import re
import os


# Sets the number of threads for multi-threading steps
multi_threads_nbr = 3
mem_mb_resources = 10000
# mem_mb_resources = (workflow.cores/3)*3000


# Sets paths for inside the container
local_path = "/data/"
ribodoc_tools = "/RiboDoc/RiboDoc/tools/"
snakemake_log_path = local_path + ".snakemake/log/"


# Wildcards definition
SAMPLES, = glob_wildcards(local_path + "fastq/{sample}.fastq.gz")
BOWTIE2 = ["1","2","3","4","rev.1","rev.2"]
HISAT2 = ["1","2","3","4","5","6","7","8"]
LENGTHS = list(map(str,range(int(config['readsLength_min']),int(config['readsLength_max'])+1)))


# Strings with minimum and maximum read lengths to be used in file names
frag_length_S = "." + LENGTHS[0]
frag_length_L = "." + LENGTHS[0] + "-" + LENGTHS[len(LENGTHS)-1]


# Change names for snakemake workflow, depending on data given
if config['UTR'] == "yes":
    counts = "htseqcount" + config['gff_cds_feature']
else:
    counts = "htseqcount"


# Check if attributes specified by the user are present in the gff file
htseq_additional = ""
htseq_header = 'ID\t'
fields = "1,2"

f = open(local_path + "database/" + config['gff'],"r")
for l in f:
    is_name = re.search("^([^\t]+\t){8}.*" + config['gff_name_attribut'],l)
    if is_name:
        name_in_gff = True
        htseq_additional = "--additional-attr " + config['gff_name_attribut']
        htseq_header = 'ID\tName\t'
        fields = "1,3"
        break
    else:
        name_in_gff = False

f.close()


rule all:
    input:
        # Call of make_fastqc rule
        expand(local_path + "RESULTS/fastqc/fastqc_before_trimming/{sample}_fastqc.html", sample=SAMPLES),
        expand(local_path + "RESULTS/fastqc/fastqc_after_trimming/{sample}.cutadapt" + frag_length_L + "_fastqc.html", sample=SAMPLES),
        expand(local_path + "RESULTS/fastqc/make_fastqc_after_outRNA_depletion/{sample}" + frag_length_L + ".no-outRNA_fastqc.html", sample=SAMPLES),

        # Call of htseqcount_transcript_utr or htseqcount_transcript rule (depends on UTR="True"|"False" in config file)
        expand(local_path + "RESULTS/htseqcount_" + config['gff_cds_feature'] + "/{sample}" + frag_length_L + ".no-outRNA." + counts + ".txt", sample=SAMPLES),

        # Call for qualitative analysis
        local_path + "RESULTS/riboWaltz/psite_offset.csv" if config['qualitative_analysis']=="ribowaltz" else expand(local_path + "RESULTS/fastqc/fastqc_before_trimming/{sample}_fastqc.html", sample=SAMPLES),
        local_path + "RESULTS/riboWaltz/transcriptome_elongated." + SAMPLES[0] + "/metaprofile_psite_-" + config['window_utr'] + "+" + config['window_cds'] + ".tiff" if config['qualitative_analysis']=="ribowaltz" else expand(local_path + "RESULTS/fastqc/fastqc_before_trimming/{sample}_fastqc.html", sample=SAMPLES),
        expand(local_path + "RESULTS/qualitativeAnalysis/graphes/periodicity/{sample}.{taille}.periodicity.start." + config['gff_cds_feature'] + ".-" + config['window_utr'] + "+" + config['window_cds'] + ".jpeg", sample=SAMPLES, taille=LENGTHS) if config['qualitative_analysis']=="trip" else expand(local_path + "RESULTS/fastqc/fastqc_before_trimming/{sample}_fastqc.html", sample=SAMPLES),
        expand(local_path + "RESULTS/qualitativeAnalysis/graphes/readsLengthRepartition/{sample}.readsLengthRepartition.jpeg", sample=SAMPLES) if config['qualitative_analysis']=="trip" else expand(local_path + "RESULTS/fastqc/fastqc_before_trimming/{sample}_fastqc.html", sample=SAMPLES),

        # Alignment statistics report
        local_path + "RESULTS/" + config['project_name'] + ".Analysis_Report.txt",

        # Call for count matrix creation for DESeq2 dans final report
        local_path + "RESULTS/DESeq2/DESeq2_by_gene/" + config['project_name'] + ".Final_report.html",
        local_path + "RESULTS/DESeq2/DESeq2_by_transcript/" + config['project_name'] + ".Final_report.html"


# When RiboDoc starts
onstart:
    # In case the user is using a Macintosh computer, hidden files are removed
    shell("find " + local_path + " -depth -name '.DS_Store' -exec rm -f {{}} \;")
    shell("find " + local_path + " -depth -name '.AppleDouble' -exec rm -rf {{}} \;")
    shell("mkdir -p " + local_path + "logs/;")
    shell("echo 'RiboDoc version : " + ribodoc_version + "' > " + local_path + "logs/RiboDoc_package_versions.txt;")
    shell("conda list >> " + local_path + "logs/RiboDoc_package_versions.txt;")

# When the jobs are all done
onsuccess:
    if config['UTR'] == "yes":
        shell("Rscript " + ribodoc_tools + "CDSvsUTR_coverage_graph.R " + config['project_name'] + ";")
    # Copy config file to keep trace of parameters used
    shell("cp " + local_path + "config.yaml " + local_path + "RESULTS/;")
    # Remove useless directories
    shell("rm -f -r " + local_path + "RESULTS/qualitativeAnalysis/bedCount/ " + local_path + "RESULTS/qualitativeAnalysis/sequenceBedCount/ " + local_path + "RESULTS/qualitativeAnalysis/*tempoR.Rout;")
    shell("rm -f -r " + local_path + "RESULTS/no-outRNA/ " + local_path + "RESULTS/cutadapt/ " + local_path + "database/*t2 " + local_path + "database/*.fai " + local_path + "*tempoR.Rout;")
    shell("cp " + snakemake_log_path + "*.snakemake.log " + local_path + "logs/ || true;")

# If anything goes wrong
onerror:
    # Copy config file to keep trace of parameters used
    shell("cp " + local_path + "config.yaml " + local_path + "RESULTS/;")
    shell("cp .snakemake/log/*.snakemake.log " + local_path + "logs/ || true;")


# Find the adapter sequence if not set in config file
rule find_adapter_sequence:
    input:
        fastq = local_path + "fastq/{sample}.fastq.gz"
    output:
        adapter = local_path + "RESULTS/adapter_lists/{sample}.txt"
    log:
        rscript = local_path + "logs/find_adapter_sequence/{sample}.rscript.log",
        sed = local_path + "logs/find_adapter_sequence/{sample}.sed.log",
        echo = local_path + "logs/find_adapter_sequence/{sample}.echo.log",
        touch = local_path + "logs/find_adapter_sequence/{sample}.touch.log"
    shell:
        "touch {output.adapter} 2> {log.touch};"
        "if [ -z " + config['adapt_sequence'] + " ]; then "
        "Rscript " + ribodoc_tools + "find_adapter_sequence.R {input.fastq} 2> {log.rscript} ;"
        "elif [ '" + config['already_trimmed'] + "' = 'no' ]; then echo " + config['adapt_sequence'] + " 1> {output.adapter} 2> {log.echo};"
        "fi;"

# Adds transcript names and gene IDs to the CDS and exon lines if possible
rule name_CDS:
    input:
        gff = local_path + "database/" + os.path.basename(config['gff'])
    output:
        gff_namedCDS = local_path + "RESULTS/annex_database/NamedCDS_" + os.path.basename(config['gff'])
    run:
        gene_id_bool = True
        if name_in_gff == True:
            db = gffutils.create_db(input.gff, ':memory:', merge_strategy='create_unique', keep_order=True)
            with open(output.gff_namedCDS, 'w') as fout:
                for d in db.directives:
                    fout.write('##{0}\n'.format(d))
                for feature in db.all_features():
                    if feature.featuretype == config['gff_cds_feature'] or feature.featuretype == "exon":
                        parent = list(db.parents(feature, level=1))
                        if len(parent) > 0:
                            parent = parent[0]
                            if parent.attributes.get(config['gff_name_attribut']) and not feature.attributes.get(config['gff_name_attribut']):
                                feature.attributes[config['gff_name_attribut']] = [i.replace("mRNA","cds") for i in parent.attributes.get(config['gff_name_attribut'])]
                                feature.attributes[config['gff_name_attribut']][0] + "_name"
                            if parent.attributes.get('ID') and not feature.attributes.get('ID'):
                                feature.attributes["ID"] = parent.attributes["ID"]
                                feature.attributes['ID'] = feature.attributes['ID'][0] + "_CDS"
                        if feature.attributes.get(config['gff_name_attribut']):
                            fout.write(str(feature) + '\n')
                    else:
                        fout.write(str(feature) + '\n')
        else:
            shell("cp {input.gff} {output.gff_namedCDS} ;")
        if gene_id_bool:
            print("'gene_id' attributes are present.")
        else:
            print("Missing at least some 'gene_id' attribute in this gff.")
        shell("sed -i -E 's/\\s/\\t/8' {output.gff_namedCDS} ;")

# Quality control of data : build of the fastqc on raw data
rule make_fastqc_before_trimming:
    input:
        local_path + "fastq/{sample}.fastq.gz"
    output:
        local_path + "RESULTS/fastqc/fastqc_before_trimming/{sample}_fastqc.zip",
        local_path + "RESULTS/fastqc/fastqc_before_trimming/{sample}_fastqc.html"
    log:
        local_path + "logs/make_fastqc_before_trimming/{sample}.log"
    benchmark:
        local_path + "benchmarks/make_fastqc_before_trimming/{sample}.benchmark.txt"
    params:
       outdir = local_path + "RESULTS/fastqc/fastqc_before_trimming/"
    shell:
        # fastqc 0.11.9
        "fastqc {input} --outdir {params.outdir} 2> {log};"

# Removes/cuts potential adapters on the reads
rule adapt_trimming:
    input:
        fastq = local_path + "fastq/{sample}.fastq.gz",
        adapt_seq = local_path + "RESULTS/adapter_lists/{sample}.txt"
    output:
        cut_fastq = local_path + "RESULTS/cutadapt/{sample}.cutadapt" + frag_length_L + ".fastq.gz"
    log:
        trim_value = local_path + "logs/adapt_trimming/{sample}_trim_value.log",
        cutadapt = local_path + "logs/adapt_trimming/{sample}_cutadapt.log",
        cutadapt_out = local_path + "logsTmp/{sample}_adapt_trimming.log"
    benchmark:
        local_path + "benchmarks/adapt_trimming/{sample}.benchmark.txt"
    resources:
        mem_mb = mem_mb_resources
    threads:
        multi_threads_nbr
    shell:
        "adapter_sequence=`cat {input.adapt_seq}` ;"
        "if [ '" + config['already_trimmed'] + "' = 'no' ]; then trim=\"-a ${{adapter_sequence}} --trimmed-only\"; else trim=''; fi 2> {log.trim_value};"
        "cutadapt ${{trim}} -e 0.125 -j {threads} --max-n=1 -m " + config['readsLength_min'] + " -M " + config['readsLength_max'] + " -o {output.cut_fastq} {input.fastq} 1>> {log.cutadapt_out} 2> {log.cutadapt} ;"

# Quality control of data : build of the fastqc after adapter trimming
rule make_fastqc_after_trimming:
    input:
        local_path + "RESULTS/cutadapt/{sample}.cutadapt" + frag_length_L + ".fastq.gz"
    output:
        local_path + "RESULTS/fastqc/fastqc_after_trimming/{sample}.cutadapt" + frag_length_L + "_fastqc.zip",
        local_path + "RESULTS/fastqc/fastqc_after_trimming/{sample}.cutadapt" + frag_length_L + "_fastqc.html"
    log:
        local_path + "logs/make_fastqc_after_trimming/{sample}.log"
    benchmark:
        local_path + "benchmarks/make_fastqc_after_trimming/{sample}.benchmark.txt"
    params:
       outdir = local_path + "RESULTS/fastqc/fastqc_after_trimming/"
    shell:
        # fastqc 0.11.9
        "fastqc {input} --outdir {params.outdir} 2> {log};"

# Builds the index of bowtie2 mapping on sequences for reads remove
rule bowtie2_build_outRNA:
    input:
        outRNA = local_path + "database/" + os.path.basename(config['fasta_outRNA'])
    output:
        expand(local_path + "RESULTS/annex_database/outRNA_bowtie2.{extb}.bt2",extb=BOWTIE2)
    params:
        outNames = local_path + "RESULTS/annex_database/outRNA_bowtie2"
    log:
        local_path + "logs/bowtie2_build_outRNA/bowtie2_build_outRNA.log"
    benchmark:
        local_path + "benchmarks/bowtie2_build_outRNA/bowtie2_build_outRNA.benchmark.txt"
    threads:
        multi_threads_nbr
    shell:
        "bowtie2-build --threads {threads} {input.outRNA} {params.outNames} &> {log} ;"

# Mapping of non-coding RNA
rule bowtie2_run_outRNA:
    input:
        expand(local_path + "RESULTS/annex_database/outRNA_bowtie2.{extb}.bt2",extb=BOWTIE2),
        fastq = local_path + "RESULTS/cutadapt/{sample}.cutadapt" + frag_length_L + ".fastq.gz"
    output:
        local_path + "RESULTS/no-outRNA/{sample}" + frag_length_L + ".no-outRNA.fastq.gz"
    log:
        bt2 = local_path + "logsTmp/{sample}_bowtie2_run_outRNA.log"
    benchmark:
        local_path + "benchmarks/bowtie2_run_outRNA/{sample}.benchmark.txt"
    resources:
        mem_mb = mem_mb_resources
    params:
        sample_names = "{sample}"
    threads:
        multi_threads_nbr
    shell:
        "bowtie2 -x " + local_path + "RESULTS/annex_database/outRNA_bowtie2 --threads {threads} -U {input.fastq} --un-gz {output} > /dev/null 2>> {log.bt2} ;"

# Quality control of data : build of the fastqc after depletin rRNA as ribosomal RNA can have an impact on the data's profiles (ATGC content)
rule make_fastqc_after_outRNA_depletion:
    input:
        local_path + "RESULTS/no-outRNA/{sample}" + frag_length_L + ".no-outRNA.fastq.gz"
    output:
        local_path + "RESULTS/fastqc/make_fastqc_after_outRNA_depletion/{sample}" + frag_length_L + ".no-outRNA_fastqc.zip",
        local_path + "RESULTS/fastqc/make_fastqc_after_outRNA_depletion/{sample}" + frag_length_L + ".no-outRNA_fastqc.html"
    log:
        local_path + "logs/make_fastqc_after_outRNA_depletion/{sample}.log"
    benchmark:
        local_path + "benchmarks/make_fastqc_after_outRNA_depletion/{sample}.benchmark.txt"
    params:
       outdir = local_path + "RESULTS/fastqc/make_fastqc_after_outRNA_depletion/"
    shell:
        # fastqc 0.11.9
        "fastqc {input} --outdir {params.outdir} 2> {log};"

# Builds the index of bowtie2 mapping for all RNA
rule bowtie2_build:
    input:
        fasta = local_path + "database/" + os.path.basename(config['fasta'])
    output:
        expand(local_path + "RESULTS/annex_database/index_bowtie2.{extb}.bt2",extb=BOWTIE2)
    log:
        local_path + "logs/bowtie2_build/bowtie2_build.log"
    benchmark:
        local_path + "benchmarks/bowtie2_build/bowtie2_build.benchmark.txt"
    threads:
        multi_threads_nbr
    params:
        outNames = local_path + "RESULTS/annex_database/index_bowtie2"
    shell:
        "bowtie2-build --threads {threads} {input.fasta} {params.outNames} &> {log} ;"

# Builds the index of hisat2 mapping for all RNA
rule hisat2_build:
    input:
        fasta = local_path + "database/" + os.path.basename(config['fasta'])
    output:
        expand(local_path + "RESULTS/annex_database/index_hisat2.{exth}.ht2",exth=HISAT2)
    log:
        local_path + "logs/hisat2_build/hisat2_build.log"
    benchmark:
        local_path + "benchmarks/hisat2_build/hisat2_build.benchmark.txt"
    threads:
        multi_threads_nbr
    params:
        outNames = local_path + "RESULTS/annex_database/index_hisat2"
    shell:
        "hisat2-build --threads {threads} {input.fasta} {params.outNames} &> {log} ;"

# Mapping of all RNA by bowtie2 and hisat2
rule run_mapping:
    input:
        expand(local_path + "RESULTS/annex_database/index_hisat2.{exth}.ht2",exth=HISAT2),
        expand(local_path + "RESULTS/annex_database/index_bowtie2.{extb}.bt2",extb=BOWTIE2),
        fastq = local_path + "RESULTS/no-outRNA/{sample}" + frag_length_L + ".no-outRNA.fastq.gz"
    output:
        fastq = temp(local_path + "RESULTS/no-outRNA/{sample}" + frag_length_L + ".no-outRNA.notAlign.fastq.gz"),
        sam_hisat2 = temp(local_path + "RESULTS/BAM_" + frag_length_L + "{sample}" + frag_length_L + ".hisat2.sam"),
        sam_bowtie2 = temp(local_path + "RESULTS/BAM_" + frag_length_L + "{sample}" + frag_length_L + ".bowtie2.sam")
    log:
        hisat2_out = local_path + "logsTmp/{sample}_run_mapping_hisat2.log",
        bowtie2_out = local_path + "logsTmp/{sample}_run_mapping_bowtie2.log"
    benchmark:
        local_path + "benchmarks/run_mapping/{sample}.benchmark.txt"
    resources:
        mem_mb = mem_mb_resources
    params:
        index_names_hisat2 = local_path + "RESULTS/annex_database/index_hisat2",
        index_names_bowtie2 = local_path + "RESULTS/annex_database/index_bowtie2",
        sample_names="{sample}"
    threads:
        multi_threads_nbr
    shell:
        "hisat2 -x {params.index_names_hisat2} --threads {threads} -U {input.fastq} --un-gz {output.fastq} -S {output.sam_hisat2} 2>> {log.hisat2_out} ;"
        "bowtie2 -x {params.index_names_bowtie2} --threads {threads} -U {output.fastq} -S {output.sam_bowtie2} 2>> {log.bowtie2_out} ;"

# Creates bam and sam files
rule samtools_filter:
    input:
        sam_hisat2 = local_path + "RESULTS/BAM_" + frag_length_L + "{sample}" + frag_length_L + ".hisat2.sam",
        sam_bowtie2 = local_path + "RESULTS/BAM_" + frag_length_L + "{sample}" + frag_length_L + ".bowtie2.sam"
    output:
        bam = local_path + "RESULTS/BAM_" + frag_length_L + "{sample}" + frag_length_L + ".bam"
    log:
        grep_header_hisat2 = local_path + "logs/samtools_filter/{sample}.grep_hisat2.log",
        grep_core_hisat2 =  local_path + "logs/samtools_filter/{sample}.grep_core_hisat2.log",
        ZS_filter_hisat2 = local_path + "logs/samtools_filter/{sample}.NH_filter_hisat2.log",
        XM_filter_hisat2 = local_path + "logs/samtools_filter/{sample}.XM_filter_hisat2.log",
        grep_core_bowtie2 =  local_path + "logs/samtools_filter/{sample}.grep_core_bowtie2.log",
        XS_filter_bowtie2 = local_path + "logs/samtools_filter/{sample}.XS_filter_bowtie2.log",
        XM_filter_bowtie2 = local_path + "logs/samtools_filter/{sample}.XM_filter_bowtie2.log",
        view_bam = local_path + "logs/samtools_filter/{sample}.view_bam.log",
        sort_bam = local_path + "logs/samtools_filter/{sample}.sort_bam.log",
        rm = local_path + "logs/samtools_filter/{sample}.rm.log"
    benchmark:
        local_path + "benchmarks/samtools_filter/{sample}.benchmark.txt"
    resources:
        mem_mb = round(mem_mb_resources / 3)
    params:
        sam = local_path + "RESULTS/BAM_" + frag_length_L + "{sample}" + frag_length_L + ".sam"
    threads:
        multi_threads_nbr
    shell:
        "set +o pipefail ;"
        "grep '^@' {input.sam_hisat2}  2> {log.grep_header_hisat2} 1> {params.sam} ;"
        "grep -v '^@' {input.sam_hisat2} 2> {log.grep_core_hisat2} | grep -v 'ZS:i:' 2> {log.ZS_filter_hisat2} | egrep -i 'XM:i:0|XM:i:1' 2> {log.XM_filter_hisat2} 1>> {params.sam} ;"
        "grep -v '^@' {input.sam_bowtie2} 2> {log.grep_core_bowtie2} | grep -v 'XS:i:' 2> {log.XS_filter_bowtie2} | egrep -i 'XM:i:0|XM:i:1' 2> {log.XM_filter_bowtie2} 1>> {params.sam} ;"
        "samtools view -@ {threads} -F 3844 -q 1 -h -b {params.sam} 2> {log.view_bam} | samtools sort -@ {threads} -o {output.bam} 2> {log.sort_bam} ;"
        "rm -rf {params.sam} 2> {log.rm};"

# Index BAMs
rule index_bam:
    input:
        bam = local_path + "RESULTS/BAM_" + frag_length_L + "{sample}" + frag_length_L + ".bam"
    output:
        bai = local_path + "RESULTS/BAM_" + frag_length_L + "{sample}" + frag_length_L + ".bam.bai"
    log:
        index_bam = local_path + "logs/index_bam/{sample}.index_bam.log"
    benchmark:
        local_path + "benchmarks/index_bam/{sample}.benchmark.txt"
    shell:
        "samtools index {input.bam} 2> {log.index_bam} ;"

# Creates an analysis report with trimming and alignment statistics
rule stats_report:
    input:
        ready = expand(rules.samtools_filter.output, sample=SAMPLES)
    output:
        stat_report = local_path + "RESULTS/" + config['project_name'] + ".Analysis_Report.txt"
    run:
        # List of interesting logs to make the report
        logs_names = ["adapt_trimming","bowtie2_run_outRNA","run_mapping_hisat2","run_mapping_bowtie2","rpkmMoyen"]
        if config['UTR'] == "no":
            logs_names = logs_names[:-1]

        # File for the statistical report
        data_report = open(output.stat_report,"w")
        for sample in SAMPLES:
            # Data treatment report creation
            data_report.write("##################\n## NEXT SAMPLE ##\n##################\n\n" + sample + "\n")
            for log in logs_names:
                data_report.write("\n" + ("#" * (len(log)+6)) + "\n## " + log + " ##\n" + ("#" * (len(log)+6)) + "\n")
                logs_files = open(local_path + "logsTmp/" + sample + "_" + log + ".log","r")
                # Keep only lines of interest from cutadapt report1,2
                i=-1
                if log=="adapt_trimming":
                    if int(2) > 1:
                        lines_to_read = range(22)
                    else:
                        lines_to_read = range(20)
                    for position, line in enumerate(logs_files):
                        if position in lines_to_read:
                            data_report.write(line)
                        else:
                            break
                else:
                    for line in logs_files:
                        data_report.write(line)
                logs_files.close()
            data_report.write("\n\n\n")
        data_report.close()

# Counts reads on each transcript
rule htseqcount:
    input:
        bam = local_path + "RESULTS/BAM_" + frag_length_L + "{sample}" + frag_length_L + ".bam",
        bai = local_path + "RESULTS/BAM_" + frag_length_L + "{sample}" + frag_length_L + ".bam.bai",
        gff = local_path + "RESULTS/annex_database/NamedCDS_" + os.path.basename(config['gff'])
    output:
        htseq_table = local_path + "RESULTS/htseqcount_" + config['gff_cds_feature'] + "/{sample}" + frag_length_L + ".no-outRNA.htseqcount.txt"
    log:
        echo = local_path + "logs/htseqcount/{sample}.echo.log",
        sed = local_path + "logs/htseqcount/{sample}.sed.log",
        htseqcount = local_path + "logs/htseqcount/{sample}.htseqcount.log",
        head = local_path + "logs/htseqcount/{sample}.head.log",
        awk = local_path + "logs/htseqcount/{sample}.awk.log"
    benchmark:
        local_path + "benchmarks/htseqcount/{sample}.benchmark.txt"
    params:
        tmp_file = local_path + "RESULTS/htseqcount_" + config['gff_cds_feature'] + "/{sample}_tmp.txt"
    threads:
        multi_threads_nbr
    shell:
        "set +o pipefail;"
        "echo -e '" + htseq_header + "' 2> {log.echo} > {output.htseq_table};"
        "sed -i '1s/$/{wildcards.sample}/' {output.htseq_table} 2> {log.sed};"
        "htseq-count -n {threads} -f bam -t " + config['gff_cds_feature'] + " -i "  + config['gff_parent_attribut'] + " " + htseq_additional + " -m intersection-strict --nonunique fraction {input.bam} {input.gff} 2> {log.htseqcount} > {params.tmp_file};"
        """head -n -5 {params.tmp_file} 2> {log.head} | awk -F '\t+' '{{if($3 >= 0) {{print($0)}} else {{printf("%s\\tNA\\t%s\\n",$1,$2)}}}}' 2> {log.awk} >> {output.htseq_table};"""
        "rm -f {params.tmp_file};"

# Counts reads on each 5prime, 3prime or CDS part of each transcript
rule htseqcount_utr:
    input:
        bam = local_path + "RESULTS/BAM_" + frag_length_L + "{sample}" + frag_length_L + ".bam",
        bai = local_path + "RESULTS/BAM_" + frag_length_L + "{sample}" + frag_length_L + ".bam.bai",
        gff = local_path + "RESULTS/annex_database/NamedCDS_" + os.path.basename(config['gff']),
        CDS = local_path + "database/" + config['CDSlength'],
        five_prime = local_path + "database/" + config['5primelength'],
        three_prime = local_path + "database/" + config['3primelength']
    output:
        counts_CDS = local_path + "RESULTS/htseqcount_" + config['gff_cds_feature'] + "/{sample}" + frag_length_L + ".no-outRNA.htseqcount" + config['gff_cds_feature'] + ".txt",
        counts_threeprime = local_path + "RESULTS/htseqcount_" + config['gff_element_three_prime_utr'] + "/{sample}" + frag_length_L + ".no-outRNA.htseqcountUTR.txt",
        counts_fiveprime = local_path + "RESULTS/htseqcount_" + config['gff_element_five_prime_utr'] + "/{sample}" + frag_length_L + ".no-outRNA.htseqcountUTR.txt"
    log:
        view = local_path + "logs/htseqcount_utr/{sample}.view.log",
        htseqcount_CDS = local_path + "logs/htseqcount_utr/{sample}.htseqcount_" + config['gff_cds_feature'] + ".log",
        join_CDS = local_path + "logs/htseqcount_utr/{sample}.join_CDS.log",
        awk_CDS = local_path + "logs/htseqcount_utr/{sample}.awk_CDS.log",
        htseqcount_3prime = local_path + "logs/htseqcount_utr/{sample}.htseqcount_3prime.log",
        join_3prime = local_path + "logs/htseqcount_utr/{sample}.join_3prime.log",
        awk_3prime = local_path + "logs/htseqcount_utr/{sample}.awk_3prime.log",
        htseqcount_5prime = local_path + "logs/htseqcount_utr/{sample}.htseqcount_5prime.log",
        join_5prime = local_path + "logs/htseqcount_utr/{sample}.join_5prime.log",
        awk_5prime = local_path + "logs/htseqcount_utr/{sample}.awk_5prime.log",
        rpkm = local_path + "logs/htseqcount_utr/{sample}.rpkm.log",
        rpkm_out = local_path + "logsTmp/{sample}_rpkmMoyen.log"
    benchmark:
        local_path + "benchmarks/htseqcount_utr/{sample}.benchmark.txt"
    threads:
        multi_threads_nbr
    params:
        sample_names="{sample}"
    shell:
        "set +o pipefail;"
        "totalReads=`samtools view -c {input.bam} 2> {log.view}`;"

        "echo -e 'ID\\tName\\t{wildcards.sample}' > {output.counts_CDS};"
        "htseq-count -n {threads} -f bam -t '" + config['gff_cds_feature'] + "' -i '" + config['gff_parent_attribut'] + "' --additional-attr '" + config['gff_name_attribut'] + "' -m intersection-strict --nonunique 'fraction' {input.bam} {input.gff} 2> {log.htseqcount_CDS} | head -n -5 >> " + local_path + "RESULTS/tmp_" + config['gff_cds_feature'] + ".{wildcards.sample}.txt;"
        "mean_RPKM_CDS=`join -1 1 -2 1 -o 1.1,1.2,2.2 -t $'\\t' {input.CDS} " + local_path + "RESULTS/tmp_" + config['gff_cds_feature'] + ".{wildcards.sample}.txt 2> {log.join_CDS} | awk -F '\t' -v totalReads=$totalReads '{{print $0,$3/($2/1000*totalReads/1000000)}}' | awk 'BEGIN{{sum=0}}{{sum=sum+$4}}END{{print sum/NR}}' 2> {log.awk_CDS}`;"
        "cat " + local_path + "RESULTS/tmp_" + config['gff_cds_feature'] + ".{wildcards.sample}.txt >> {output.counts_CDS};"

        "echo -e 'ID\\tName\\t{wildcards.sample}' > {output.counts_threeprime};"
        "htseq-count -n {threads} -f bam -t '" + config['gff_element_three_prime_utr'] + "' -i '" + config['gff_parent_attribut'] + "' --additional-attr '" + config['gff_name_attribut'] + "' -m intersection-strict  --nonunique 'fraction' {input.bam} {input.gff} 2> {log.htseqcount_3prime} | head -n -5 >> " + local_path + "RESULTS/tmp_" + config['gff_element_three_prime_utr'] + ".{wildcards.sample}.txt;"
        "mean_RPKM_3prime=`join -1 1 -2 1 -o 1.1,1.2,2.2 -t $'\\t' {input.three_prime} " + local_path + "RESULTS/tmp_" + config['gff_element_three_prime_utr'] + ".{wildcards.sample}.txt 2> {log.join_3prime} | awk -F '\t' -v totalReads=$totalReads '{{print $0,$3/($2/1000*totalReads/1000000)}}' | awk 'BEGIN{{sum=0}}{{sum=sum+$4}}END{{print sum/NR}}' 2> {log.awk_3prime}`;"
        "cat " + local_path + "RESULTS/tmp_" + config['gff_element_three_prime_utr'] + ".{wildcards.sample}.txt >> {output.counts_threeprime};"

        "echo -e 'ID\\tName\\t{wildcards.sample}' > {output.counts_fiveprime};"
        "htseq-count -n {threads} -f bam -t '" + config['gff_element_five_prime_utr'] + "' -i '" + config['gff_parent_attribut'] + "' --additional-attr '" + config['gff_name_attribut'] + "' -m intersection-strict  --nonunique 'fraction' {input.bam} {input.gff} 2> {log.htseqcount_5prime} | head -n -5 >> " + local_path + "RESULTS/tmp_" + config['gff_element_five_prime_utr'] + ".{wildcards.sample}.txt;"
        "mean_RPKM_5prime=`join -1 1 -2 1 -o 1.1,1.2,2.2 -t $'\\t' {input.five_prime} " + local_path + "RESULTS/tmp_" + config['gff_element_five_prime_utr'] + ".{wildcards.sample}.txt 2> {log.join_5prime} | awk -F '\t' -v totalReads=$totalReads '{{print $0,$3/($2/1000*totalReads/1000000)}}' | awk 'BEGIN{{sum=0}}{{sum=sum+$4}}END{{print sum/NR}}' 2> {log.awk_5prime}`;"
        "cat " + local_path + "RESULTS/tmp_" + config['gff_element_five_prime_utr'] + ".{wildcards.sample}.txt >> {output.counts_fiveprime};"

        "rm -f " + local_path + "RESULTS/tmp_" + config['gff_cds_feature'] + ".{wildcards.sample}.txt;"
        "rm -f " + local_path + "RESULTS/tmp_" + config['gff_element_three_prime_utr'] + ".{wildcards.sample}.txt;"
        "rm -f " + local_path + "RESULTS/tmp_" + config['gff_element_five_prime_utr'] + ".{wildcards.sample}.txt;"

        "echo 'Mean RPKM' 1>> {log.rpkm_out} 2> {log.rpkm};"
        "echo '" + config['gff_cds_feature'] + " : '$mean_RPKM_CDS 1>> {log.rpkm_out} 2>> {log.rpkm};"
        "echo '5prime : '$mean_RPKM_5prime 1>> {log.rpkm_out} 2>> {log.rpkm};"
        "echo '3prime : '$mean_RPKM_3prime 1>> {log.rpkm_out} 2>> {log.rpkm};"

# Creates the row names (genes/transcript names) of the count matrix
rule count_matrix_initialization:
    input:
        ready = expand(rules.htseqcount_utr.output, sample=SAMPLES) if config['UTR']=="yes" else expand(rules.htseqcount.output, sample=SAMPLES),
        counts = local_path + "RESULTS/htseqcount_" + config['gff_cds_feature'] + "/" + SAMPLES[0] + "" + frag_length_L + ".no-outRNA." + counts + ".txt"
    output:
        transcript_list = local_path + "RESULTS/DESeq2/names_correspondence_list.txt"
    log:
        header = "logs/count_matrix_initialization/head.log",
        id_name = "logs/count_matrix_initialization/id_name.log",
        sort = local_path + "logs/count_matrix_initialization/sort.log"
    benchmark:
        local_path + "benchmarks/count_matrix_initialization/count_matrix_initialization.benchmark.txt"
    run:
        shell("set +o pipefail;")
        if name_in_gff == True:
            shell("""
            echo -e 'ID\\tTranscript_name\\tGene_name' 2> {log.header} > {output.transcript_list};
            tail +2 {input.counts} | cut -f 1,2 2> {log.id_name} | LC_COLLATE=C sort 2> {log.sort} >> {output.transcript_list};
            sed -i -E 's/\\t([^\\t]+)(-[0-9]{{3}})/\\t\\1\\2\\t\\1/' {output.transcript_list};
            sed -i -E 's/^([^\\t]+)\\t([^\t]+)$/\\1\\t\\2\\t\\2/' {output.transcript_list};
            """)
        else:
            shell("""
            echo -e 'ID' 2> {log.header} > {output.transcript_list};
            tail +2 {input.counts} | cut -f 1,2 2> {log.id_name} | LC_COLLATE=C sort 2> {log.sort} >> {output.transcript_list};
            """)

# Adds the read counts of each sample to the matrix
rule count_matrix_creation:
    input:
        transcript_list = local_path + "RESULTS/DESeq2/names_correspondence_list.txt"
    output:
        counts_matrix = local_path + "RESULTS/DESeq2/count_matrix_by_transcript_unnamed.txt"
    log:
        local_path + "logs/count_matrix_creation/count_matrix.log"
    benchmark:
        local_path + "benchmarks/count_matrix_creation/count_matrix_creation.benchmark.txt"
    params:
        tmp_file = local_path + "RESULTS/DESeq2/tmp.txt"
    run:
        shell("cut -f 1 {input.transcript_list} 2> {log} > {output.counts_matrix};")
        for sample in SAMPLES:
            if name_in_gff == True:
                fields = "1,3"
            else:
                fields = "1,2"
            shell("cut -f " + fields + " " + local_path + "RESULTS/htseqcount_" + config['gff_cds_feature'] + "/" + sample + frag_length_L + ".no-outRNA." + counts + ".txt | LC_COLLATE=C join --header -t $'\t' {output.counts_matrix} - > {params.tmp_file} 2>> {log};")
            shell("cat {params.tmp_file} > {output.counts_matrix};")
            shell("rm -f {params.tmp_file};")

# Performs differential analysis
rule DESeq2_analysis_gene:
    input:
        transcript_list = local_path + "RESULTS/DESeq2/names_correspondence_list.txt",
        counts_matrix = local_path + "RESULTS/DESeq2/count_matrix_by_transcript_unnamed.txt"
    output:
        complete = local_path + "RESULTS/DESeq2/DESeq2_by_gene/gene_complete.txt",
        up = local_path + "RESULTS/DESeq2/DESeq2_by_gene/gene_up.txt",
        down = local_path + "RESULTS/DESeq2/DESeq2_by_gene/gene_down.txt",
        report = local_path + "RESULTS/DESeq2/DESeq2_by_gene/" + config['project_name'] + ".Final_report.html"
    log:
        control = local_path + "logs/DESeq2_analysis_gene/header_control.log",
        deseq2 = local_path + "logs/DESeq2_analysis_gene/DESeq2_analysis.log"
    benchmark:
        local_path + "benchmarks/DESeq2_analysis_gene/DESeq2_analysis.benchmark.txt"
    params:
        reportPath = local_path + "RESULTS/DESeq2/DESeq2_by_gene/",
        reportName = config['project_name'] + ".Final_report.html"
    shell:
        "set +o pipefail;"
        "[ `head -2 {input.counts_matrix} | cut -f 1 | uniq | wc -l` == 1 ] && sed -i -e '2d' {input.counts_matrix};"
        "Rscript -e \"rmarkdown::render('" + ribodoc_tools + "DESeq2_analysis_by_gene.Rmd', output_format='html_document', output_file='{params.reportName}', output_dir='{params.reportPath}', intermediates_dir = '{params.reportPath}')\" 2> {log.deseq2};"

rule DESeq2_analysis_transcript:
    input:
        transcript_list = local_path + "RESULTS/DESeq2/names_correspondence_list.txt",
        counts_matrix = local_path + "RESULTS/DESeq2/count_matrix_by_transcript_unnamed.txt"
    output:
        complete = local_path + "RESULTS/DESeq2/DESeq2_by_transcript/transcript_complete.txt",
        up = local_path + "RESULTS/DESeq2/DESeq2_by_transcript/transcript_up.txt",
        down = local_path + "RESULTS/DESeq2/DESeq2_by_transcript/transcript_down.txt",
        report = local_path + "RESULTS/DESeq2/DESeq2_by_transcript/" + config['project_name'] + ".Final_report.html"
    log:
        control = local_path + "logs/DESeq2_analysis_transcript/header_control.log",
        deseq2 = local_path + "logs/DESeq2_analysis_transcript/DESeq2_analysis.log"
    benchmark:
        local_path + "benchmarks/DESeq2_analysis_transcript/DESeq2_analysis.benchmark.txt"
    params:
        reportPath = local_path + "RESULTS/DESeq2/DESeq2_by_transcript/",
        reportName = config['project_name'] + ".Final_report.html"
    shell:
        "set +o pipefail;"
        "[ `head -2 {input.counts_matrix} | cut -f 1 | uniq | wc -l` == 1 ] && sed -i -e '2d' {input.counts_matrix};"
        "Rscript -e \"rmarkdown::render('" + ribodoc_tools + "DESeq2_analysis_by_transcript.Rmd', output_format='html_document', output_file='{params.reportName}', output_dir='{params.reportPath}', intermediates_dir = '{params.reportPath}')\" 2> {log.deseq2};"


# Quality controls :
# riboWaltz pipeline
# In case there are no UTRs in the original GFF, call for ORFget functions
rule ORFget:
    input:
        fasta = local_path + "database/" + os.path.basename(config['fasta']),
        gff = local_path + "RESULTS/annex_database/NamedCDS_" + os.path.basename(config['gff'])
    output:
        fasta = local_path + "RESULTS/annex_database/transcriptome_elongated.nfasta",
        gff = local_path + "RESULTS/annex_database/transcriptome_elongated.gff",
        gff_with_genes = local_path + "RESULTS/annex_database/transcriptome_elongated_with_gene_features.gff"
    log:
        orf_get = local_path + "logs/ORFget/orf_get.log",
        mv = local_path + "logs/ORFget/mv.log",
        awk = local_path + "logs/ORFget/awk.log"
    benchmark:
        local_path + "benchmarks/ORFget/ORFget.benchmark.txt"
    params:
        path = local_path + "RESULTS/annex_database/transcriptome"
    shell:
        "python3 " + ribodoc_tools + "ORFget.py -fna {input.fasta} -gff {input.gff} -features_include " + config['gff_cds_feature'] + " -name_attribute " + config['gff_name_attribut'] + " -o {params.path} -type nucl -elongate 50 -check 2> {log.orf_get};"
        "mv {output.gff} {output.gff_with_genes} 2> {log.mv};"
        "awk '$3 !~ /gene/' {output.gff_with_genes} > {output.gff} 2> {log.awk};"

# Create GTF file for riboWaltz
rule transcriptome_construction_gtf:
    input:
        fasta = local_path + "RESULTS/annex_database/transcriptome_elongated.nfasta",
        gff_with_genes = local_path + "RESULTS/annex_database/transcriptome_elongated_with_gene_features.gff"
    output:
        fasta = local_path + "RESULTS/annex_database/transcriptome_elongated.exons_" + os.path.basename(config['fasta']),
        gtf = local_path + "RESULTS/annex_database/transcriptome_elongated.exons_" + os.path.basename(config['gff']) + ".gtf"
    log:
        samtools_index = local_path + "logs/transcriptome_construction_gtf/samtools_index.log",
        gffread_gtf = local_path + "logs/transcriptome_construction_gtf/gffread.log",
        sed = local_path + "logs/transcriptome_construction_gtf/sed.log",
        awk = local_path + "logs/transcriptome_construction_gtf/awk.log"
    benchmark:
        local_path + "benchmarks/transcriptome_construction_gtf/transcriptome_construction_gtf.benchmark.txt"
    params:
        tmp_gtf = local_path + "RESULTS/annex_database/transcriptome_elongated.exons_tmp.gtf"
    shell:
        "samtools faidx {input.fasta} 2> {log.samtools_index} ;"
        "gffread -F -T -w {output.fasta} -o {params.tmp_gtf} -g {input.fasta} {input.gff_with_genes} 2> {log.gffread_gtf} ;"
        "sed -i 's/description[^\;]*\;//' {params.tmp_gtf} 2>> {log.sed} ;"
        "sed -i 's/\\t[A-Z]*[_]*gene_segment\\t/\\ttranscript\\t/' {params.tmp_gtf} 2>> {log.sed} ;"
        """awk -F '\\t' '{{if(NF<=9) {{print($0);}} else {{for(field=1;field<9;field++) {{printf("%s\\t",$field);}} for(field=9;field<=NF;field++) {{printf("%s ",$field);}} printf("\\n");}}}}' {params.tmp_gtf} > {output.gtf} 2>> {log.awk} ;"""
        "rm -f {params.tmp_gtf};"
        "sed -i 's/\\s$//' {output.gtf} 2>> {log.sed} ;"

# Builds the index of bowtie2 mapping for all RNA
rule bowtie2_build_transcriptome:
    input:
        fasta = local_path + "RESULTS/annex_database/transcriptome_elongated.nfasta"
    output:
        expand(local_path + "RESULTS/annex_database/transcriptome_elongated.transcriptome_index_bowtie2.{extb}.bt2",extb=BOWTIE2)
    log:
        local_path + "logs/bowtie2_build_transcriptome/bowtie2_build.log"
    benchmark:
        local_path + "benchmarks/bowtie2_build_transcriptome/transcriptome_index_bowtie2.benchmark.txt"
    params:
        index_names = local_path + "RESULTS/annex_database/transcriptome_elongated.transcriptome_index_bowtie2"
    threads:
        multi_threads_nbr
    shell:
        "bowtie2-build --threads {threads} {input.fasta} {params.index_names} &> {log} ;"

# Builds the index of hisat2 mapping for all RNA
rule hisat2_build_transcriptome:
    input:
        fasta = local_path + "RESULTS/annex_database/transcriptome_elongated.nfasta"
    output:
        expand(local_path + "RESULTS/annex_database/transcriptome_elongated.transcriptome_index_hisat2.{exth}.ht2",exth=HISAT2)
    log:
        local_path + "logs/hisat2_build_transcriptome/hisat2_build.log"
    benchmark:
        local_path + "benchmarks/hisat2_build_transcriptome/hisat2_build_transcriptome.benchmark.txt"
    params:
        index_names = local_path + "RESULTS/annex_database/transcriptome_elongated.transcriptome_index_hisat2"
    threads:
        multi_threads_nbr
    shell:
        "hisat2-build --threads {threads} {input.fasta} {params.index_names} &> {log} ;"

# Performs mapping on transcriptome
rule run_mapping_transcriptome:
    input:
        expand(local_path + "RESULTS/annex_database/transcriptome_elongated.transcriptome_index_hisat2.{exth}.ht2",exth=HISAT2),
        expand(local_path + "RESULTS/annex_database/transcriptome_elongated.transcriptome_index_bowtie2.{extb}.bt2",extb=BOWTIE2),
        fastq = local_path + "RESULTS/no-outRNA/{sample}" + frag_length_L + ".no-outRNA.fastq.gz"
    output:
        sam_hisat2 = temp(local_path + "RESULTS/BAM_transcriptome_" + frag_length_L + "/{sample}" + frag_length_L + ".hisat2.sam"),
        sam_bowtie2 = temp(local_path + "RESULTS/BAM_transcriptome_" + frag_length_L + "/{sample}" + frag_length_L + ".bowtie2.sam"),
        fastq = temp(local_path + "RESULTS/no-outRNA/{sample}" + frag_length_L + ".no-outRNA.notAlign.transcriptome.fastq.gz")
    log:
        hisat2_out = local_path + "logs/run_mapping_transcriptome/{sample}_run_mapping_transcriptome_hisat2.log",
        bowtie2_out = local_path + "logs/run_mapping_transcriptome/{sample}_run_mapping_transcriptome_bowtie2.log"
    benchmark:
        local_path + "benchmarks/run_mapping_transcriptome/{sample}.benchmark.txt"
    resources:
        mem_mb = mem_mb_resources
    params:
        index_names_hisat2 = local_path + "RESULTS/annex_database/transcriptome_elongated.transcriptome_index_hisat2",
        index_names_bowtie2 = local_path + "RESULTS/annex_database/transcriptome_elongated.transcriptome_index_bowtie2",
        sample_names = "{sample}"
    threads:
        multi_threads_nbr
    shell:
        "hisat2 -x {params.index_names_hisat2} --threads {threads} -k 10 -U {input.fastq} --un-gz {output.fastq} -S {output.sam_hisat2} 2>> {log.hisat2_out} ;"
        "bowtie2 -x {params.index_names_bowtie2} --threads {threads} -k 10 -U {output.fastq} -S {output.sam_bowtie2} 2>> {log.bowtie2_out} ;"

# Creates bam and sam files
rule samtools_filter_transcriptome:
    input:
        sam_hisat2 = local_path + "RESULTS/BAM_transcriptome_" + frag_length_L + "/{sample}" + frag_length_L + ".hisat2.sam",
        sam_bowtie2 = local_path + "RESULTS/BAM_transcriptome_" + frag_length_L + "/{sample}" + frag_length_L + ".bowtie2.sam"
    output:
        bam = local_path + "RESULTS/BAM_transcriptome_" + frag_length_L + "/transcriptome_elongated.{sample}" + frag_length_L + ".bam"
    log:
        grep_header_hisat2 = local_path + "logs/samtools_filter_transcriptome/{sample}.grep_hisat2.log",
        uniq_header = local_path + "logs/samtools_filter_transcriptome/{sample}.uniq_header.log",
        grep_core_hisat2 =  local_path + "logs/samtools_filter_transcriptome/{sample}.grep_core_hisat2.log",
        XM_filter_hisat2 = local_path + "logs/samtools_filter_transcriptome/{sample}.XM_filter_hisat2.log",
        XM_filter_bowtie2 = local_path + "logs/samtools_filter/{sample}.XM_filter_bowtie2.log",
        grep_core_bowtie2 =  local_path + "logs/samtools_filter_transcriptome/{sample}.grep_core_bowtie2.log",
        view_bam = local_path + "logs/samtools_filter_transcriptome/{sample}.view_bam.log",
        sort_bam = local_path + "logs/samtools_filter_transcriptome/{sample}.sort_bam.log"
    benchmark:
        local_path + "benchmarks/samtools_filter_transcriptome/{sample}.benchmark.txt"
    resources:
        mem_mb = round(mem_mb_resources / 3)
    params:
        sample = "{sample}",
        sam = local_path + "RESULTS/BAM_transcriptome_" + frag_length_L + "/{sample}" + frag_length_L + ".sam"
    threads:
        multi_threads_nbr
    shell:
        "set +o pipefail ;"
        "grep '^@' {input.sam_hisat2}  2> {log.grep_header_hisat2} | uniq 2> {log.uniq_header} 1> {params.sam} ;"
        "grep -v '^@' {input.sam_hisat2} 2> {log.grep_core_hisat2} | egrep -i 'XM:i:0|XM:i:1' 2> {log.XM_filter_hisat2} 1>> {params.sam} ;"
        "grep -v '^@' {input.sam_bowtie2} 2> {log.grep_core_bowtie2} | egrep -i 'XM:i:0|XM:i:1' 2> {log.XM_filter_bowtie2} 1>> {params.sam} ;"
        "samtools view -@ {threads} -F 3588 -h -b {params.sam} 2> {log.view_bam} | samtools sort -@ {threads} -o {output.bam} 2> {log.sort_bam} ;"
        "rm -f {params.sam} ;"

# Index BAMs
rule index_bam_transcriptome:
    input:
        bam = local_path + "RESULTS/BAM_transcriptome_" + frag_length_L + "/transcriptome_elongated.{sample}" + frag_length_L + ".bam"
    output:
        bai = local_path + "RESULTS/BAM_transcriptome_" + frag_length_L + "/transcriptome_elongated.{sample}" + frag_length_L + ".bam.bai"
    log:
        index_bam = local_path + "logs/index_bam_transcriptome/{sample}.index_bam.log"
    benchmark:
        local_path + "benchmarks/index_bam_transcriptome/{sample}.benchmark.txt"
    shell:
        "samtools index {input.bam} 2> {log.index_bam} ;"

# Performs qualitative analysis with riboWaltz
rule riboWaltz_transcriptome:
    input:
        transcriptome_gtf = local_path + "RESULTS/annex_database/transcriptome_elongated.exons_" + os.path.basename(config['gff']) + ".gtf",
        transcriptome_bam = expand(rules.samtools_filter_transcriptome.output, sample=SAMPLES)
    output:
        psite_table = local_path + "RESULTS/riboWaltz/psite_offset.csv",
        metaprofile = local_path + "RESULTS/riboWaltz/transcriptome_elongated." + SAMPLES[0] + "/metaprofile_psite_-" + config['window_utr'] + "+" + config['window_cds'] + ".tiff"
    log:
        periodicity = local_path + "logs/riboWaltz_transcriptome/riboWaltz.log"
    resources:
        mem_mb = mem_mb_resources
    benchmark:
        local_path + "benchmarks/riboWaltz_transcriptome/riboWaltz.benchmark.txt"
    shell:
        "touch {output.psite_table};"
        "Rscript " + ribodoc_tools + "periodicity_riboWaltz_transcriptome.R {input.transcriptome_gtf} 2> {log} ;"
        "rm -f " + local_path + "Rplots.pdf ;"


# TRiP pipeline
# Divisions of SAM file according to read length and turns it into BAM
rule quality_controls_bamDivision:
    input:
        bam = local_path + "RESULTS/BAM_" + frag_length_L + "{sample}" + frag_length_L + ".bam",
        gff = local_path + "RESULTS/annex_database/NamedCDS_" + config['gff']
    output:
        bam = local_path + "RESULTS/qualitativeAnalysis/bamDivision/{sample}.{taille}.uniq.sort.bam",
        bai = local_path + "RESULTS/qualitativeAnalysis/bamDivision/{sample}.{taille}.uniq.sort.bam.bai"
    log:
        local_path + "logs/quality_controls_bamDivision/{sample}.{taille}.BamDivision.log"
    benchmark:
        local_path + "benchmarks/quality_controls_bamDivision/{sample}.{taille}.benchmark.txt"
    threads:
        multi_threads_nbr
    params:
        sample_names = "{sample}",
        read_length = "{taille}"
    shell:
        "bash " + ribodoc_tools + "BamDivision.sh -N {params.sample_names} -l {params.read_length} -B {input.bam} -T {threads} -o " + local_path + "RESULTS/qualitativeAnalysis/ 2> {log};"

# Creates bed files from fasta files
rule quality_controls_bedcount:
    input:
        bam = local_path + "RESULTS/qualitativeAnalysis/bamDivision/{sample}.{taille}.uniq.sort.bam",
        bai = local_path + "RESULTS/qualitativeAnalysis/bamDivision/{sample}.{taille}.uniq.sort.bam.bai",
        fasta = local_path + "database/" + config['fasta']
    output:
        sequenceBedCount = local_path + "RESULTS/qualitativeAnalysis/sequenceBedCount/{sample}.{taille}.count.sequence.bed",
        readsLengthRepartitionBed = temp(local_path + "RESULTS/qualitativeAnalysis/readsLengthRepartition/{sample}.{taille}.bed"),
        bed = temp(local_path + "RESULTS/qualitativeAnalysis/bedCount/{sample}.{taille}.count.bed")
    log:
        local_path + "logs/quality_controls_bedcount/{sample}.{taille}.readsLengthRepartition.log"
    benchmark:
        local_path + "benchmarks/quality_controls_bedcount/{sample}.{taille}.benchmark.txt"
    params:
        sample_names = "{sample}",
        read_length = "{taille}"
    shell:
        "bash " + ribodoc_tools + "readsLengthRepartition.sh -N {params.sample_names} -l {params.read_length} -F {input.fasta} -D " + local_path + "RESULTS/qualitativeAnalysis/bamDivision/ -O " + local_path + "RESULTS/qualitativeAnalysis/ 2> {log};"

# Outputs the number of reads on each reads length
rule quality_controls_readsLengthRepartition:
    input:
        expand(rules.quality_controls_bedcount.output.readsLengthRepartitionBed, sample=SAMPLES, taille=LENGTHS)
    output:
        local_path + "RESULTS/qualitativeAnalysis/readsLengthRepartition/{sample}.readsLengthRepartition.txt"
    log:
        wc = local_path + "logs/quality_controls_readsLengthRepartition/{sample}.wc.log",
        sed1 = local_path + "logs/quality_controls_readsLengthRepartition/{sample}.sed1.log",
        awk = local_path + "logs/quality_controls_readsLengthRepartition/{sample}.awk.log",
        sed2 = local_path + "logs/quality_controls_readsLengthRepartition/{sample}.sed2.log",
        head = local_path + "logs/quality_controls_readsLengthRepartition/{sample}.head.log"
    benchmark:
        local_path + "benchmarks/quality_controls_readsLengthRepartition/{sample}.benchmark.txt"
    params:
        path = local_path + "RESULTS/quality_controls_readsLengthRepartition/readsLengthRepartition/",
        sample_names = "{sample}"
    shell:
        "set +o pipefail;"
        "wc -l {params.path}{params.sample_names}* 2> {log.wc} | sed 's/\./ /g' 2> {log.sed1} | awk -F ' ' '{{print $(NF-1),$1}}' 2> {log.awk} | sed 's/ /\\t/g' 2> {log.sed2} | head -n -1 2> {log.head}  > {output};"

# Looks how many reads start on each base to find if there is a periodicity signal
rule quality_controls_periodicity:
    input:
        bed = local_path + "RESULTS/qualitativeAnalysis/readsLengthRepartition/{sample}.{taille}.bed",
        gff = local_path + "RESULTS/annex_database/NamedCDS_" + config['gff']
    output:
        start = local_path + "RESULTS/qualitativeAnalysis/periodicity/{sample}.{taille}.periodicity.start." + config['gff_cds_feature'] + ".-" + config['window_utr'] + "+" + config['window_cds'] + ".txt",
        stop = local_path + "RESULTS/qualitativeAnalysis/periodicity/{sample}.{taille}.periodicity.stop." + config['gff_cds_feature'] + ".-" + config['window_cds'] + "+" + config['window_utr'] + ".txt"
    params:
        sample_names = "{sample}",
        read_length = "{taille}"
    log:
        start = local_path + "logs/quality_controls_periodicity/{sample}.{taille}.log",
        stop = local_path + "logs/quality_controls_periodicity/{sample}.{taille}.log"
    benchmark:
        local_path + "benchmarks/quality_controls_periodicity/{sample}.{taille}.benchmark.txt"
    shell:
        "bash " + ribodoc_tools + "periodicity.sh -N {params.sample_names} -l {params.read_length} -G {input.gff} -D " + local_path + "RESULTS/qualitativeAnalysis/bedCount/ -p 'start' -t '" + config['gff_cds_feature'] + "' -m " + config['window_utr'] + " -M " + config['window_cds'] + " -r 'metagene' -O " + local_path + "RESULTS/qualitativeAnalysis/ 2> {log.start};"
        "bash " + ribodoc_tools + "periodicity.sh -N {params.sample_names} -l {params.read_length} -G {input.gff} -D " + local_path + "RESULTS/qualitativeAnalysis/bedCount/ -p 'stop' -t '" + config['gff_cds_feature'] + "' -m " + config['window_cds'] + " -M " + config['window_utr'] + " -r 'metagene' -O " + local_path + "RESULTS/qualitativeAnalysis/ 2> {log.stop};"

# Creates graphs of reads length repartition
rule graphs_length:
    input:
        length = rules.quality_controls_readsLengthRepartition.output
    output:
        length = local_path + "RESULTS/qualitativeAnalysis/graphes/readsLengthRepartition/{sample}.readsLengthRepartition.jpeg"
    params:
        sample_name = "{sample}"
    log:
        bash = local_path + "logs/graphs_length/{sample}.generationGraph_length.log"
    benchmark:
        local_path + "benchmarks/graphs_length/{sample}.benchmark.txt"
    shell:
        "bash " + ribodoc_tools + "generationGraph_length.sh -N {params.sample_name} 2> {log.bash};"

# Creates periodicity graphs
rule graphs_periodicity:
    input:
        perio = rules.quality_controls_periodicity.output
    output:
        perioStart = local_path + "RESULTS/qualitativeAnalysis/graphes/periodicity/{sample}.{taille}.periodicity.start." + config['gff_cds_feature'] + ".-" + config['window_utr'] + "+" + config['window_cds'] + ".jpeg",
        perioStop = local_path + "RESULTS/qualitativeAnalysis/graphes/periodicity/{sample}.{taille}.periodicity.stop." + config['gff_cds_feature'] + ".-" + config['window_cds'] + "+" + config['window_utr'] + ".jpeg"
    params:
        sample_name = "{sample}",
        read_length = "{taille}"
    log:
        bash = local_path + "logs/graphs_periodicity/{sample}.{taille}.generationGraph_perio.log"
    benchmark:
        local_path + "benchmarks/graphs_periodicity/{sample}.{taille}.benchmark.txt"
    shell:
        "bash " + ribodoc_tools + "generationGraph_perio.sh -N {params.sample_name} -l {params.read_length} -t '" + config['gff_cds_feature'] + "' -m " + config['window_utr'] + " -M " + config['window_cds'] + " 2> {log.bash};"
